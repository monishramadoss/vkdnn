
#version 460
#extension GL_EXT_shader_explicit_arithmetic_types_float32 : enable 
#define NUM_THREADS 128
#define WARPSIZE 32
#define BN NUM_THREADS
#define BM 64
#define BK 16
#define WN 64
#define WM 32
#define TN 4
#define TM 4
#define WNINTER 1
#define WMINTER 4
#define WSUBN 64
#define WSUBM 0
#define NUM_WARPS 4

layout(local_size_x = NUM_THREADS) in;
layout(push_constant) uniform pushBlock {
    uint m;
    uint k;
    uint n;
    float alpha;
    float beta;
};

layout(binding=0) buffer buf_0 { float tensor_0[]; };
layout(binding=1) buffer buf_1 { float tensor_1[]; };
layout(binding=2) buffer buf_2 { float tensor_2[]; };


float regM[WMINTER * TM];
float regN[WNINTER * TN];
float threadResults[WMINTER * TM * WNINTER * TN];

shared float As[BM * BK];
shared float Bs[BK * BN];



    
void loadRegs(uint rowStrideA, uint rowStrideB, uint a, uint b, uint as, uint bs, uint innerRowA, uint innerColA, uint innerRowB, uint innerColB){
    //loads stuff
    for(uint offset = 0; offset+rowStrideA <= BM; offset += rowStrideA) {
        uint xIdx = a + (innerRowA + offset) * k + innerColA * 4;
        vec4 tmp = vec4(tensor_0[xIdx + 0], tensor_0[xIdx + 1], tensor_0[xIdx + 2], tensor_0[xIdx + 3]);
        As[as + ((innerColA * 4) + 0) * BM + innerRowA + offset] = tmp.x;
        As[as + ((innerColA * 4) + 1) * BM + innerRowA + offset] = tmp.y;
        As[as + ((innerColA * 4) + 2) * BM + innerRowA + offset] = tmp.z;
        As[as + ((innerColA * 4) + 3) * BM + innerRowA + offset] = tmp.w;
    }

    for(uint offset = 0; offset+rowStrideB <= BK; offset += rowStrideB) {
        uint mIdx = b + (innerRowB + offset) * n + innerColB * 4;
        vec4 tmp = vec4(tensor_1[mIdx + 0], tensor_1[mIdx + 1], tensor_1[mIdx + 2], tensor_1[mIdx + 3]);
        Bs[bs + (innerRowB + offset) * BN + innerColB * 4 + 0] = tmp.x;
        Bs[bs + (innerRowB + offset) * BN + innerColB * 4 + 1] = tmp.y;
        Bs[bs + (innerRowB + offset) * BN + innerColB * 4 + 2] = tmp.z;
        Bs[bs + (innerRowB + offset) * BN + innerColB * 4 + 3] = tmp.w;
    }
}

void processFromSmem(uint as, uint bs uint warpRow, uint warpCol, uint threadRowWarp, uint threadColWarp){
    for(uint dotIdx = 0; dotIdx < BK; ++dotIdx){
        for(uint wSubRowIdx = 0; wSubRowIdx < WMINTER; ++wSubRowIdx){
            for(uint i = 0; i < TM; ++i)
                regM[wSubRowIdx * TM + i] = As[as + (dotIdx * BM) + (warpRow * WM) + (wSubRowIdx * WSUBM) + (threadRowWarp * TM) + i];
        }

        for(uint wSubColIdx = 0; wSubColIdx < WNINTER; ++wSubColIdx){
            for(uint i = 0; i < TN; ++i)
                regN[wSubColIdx * TN + i] = Bs[bs + (dotIdx * BN) + (warpCol * WN) + (wSubColIdx * WSUBN) + (threadColWarp * TN) + i];
        }

        for(uint wSubRowIdx = 0; wSubRowIdx < WMINTER; ++wSubRowIdx){
            for(uint wSubColIdx = 0; wSubColIdx < WNINTER; ++wSubColIdx){

                for (uint resIdxM = 0; resIdxM < TM; ++resIdxM) {
                    for (uint resIdxN = 0; resIdxN < TN; ++resIdxN) {

                        uint reg_idx = (wSubRowIdx * TM + resIdxM) * (WNINTER * TN) + (wSubColIdx * TN) + resIdxN;
                        threadResults[reg_idx] = fma(regM[wSubRowIdx * TM + resIdxM], regN[wSubColIdx * TN + resIdxN], threadResults[reg_idx]);
                    }
                }
            }
        }
    }
}

void main() {
    uint cRow = gl_WorkGroupID.y;
    uint cCol = gl_WorkGroupID.x;

    uint tid = gl_LocalInvocationID.x;

    uint warpIdx = tid / WARPSIZE;
    uint warpCol = warpIdx % (BN / WN);
    uint warpRow = warpIdx / (BN / WN);

    uint threadIdxWarp = tid % WARPSIZE;
    uint threadColWarp = threadIdxWarp % (WSUBN / TN);
    uint threadRowWarp = threadIdxWarp / (WSUBN / TN);

    uint a = cRow * BM * k;
    uint b = cCol * BN;
    uint c = ((cRow * BM) + (warpRow * WM)) * n + (cCol * BN) + (warpCol * WN);
    

    uint innerRowA = tid / (BK / 4);
    uint innerColA = tid % (BK / 4);
    uint rowStrideA = (NUM_THREADS * 4) / BK;
    
    uint innerRowB = tid / (BN / 4);
    uint innerColB = tid % (BN / 4);
    uint rowStrideB = NUM_THREADS / (BN / 4);


    for(uint blkIdx = 0; blkIdx < k; blkIdx += BK) {
        loadRegs(rowStrideA, rowStrideB, a, b, 0, 0, innerRowA, innerColA, innerRowB, innerColB);
        //memoryBarrierShared();
        barrier();
        processFromSmem(0, 0, warpRow, warpCol, threadRowWarp, threadColWarp);

        a += BK;
        b += BK * n;
       
        barrier();
    }

    for(uint wSubRowIdx = 0; wSubRowIdx < WMINTER; ++wSubRowIdx) {
        for(uint wSubColIdx = 0; wSubColIdx < WNINTER; ++wSubColIdx) {
            uint c_interm = c + (wSubRowIdx * WSUBM) * n + wSubColIdx * WSUBN;

            for(uint resIdxM = 0; resIdxM < TM; ++resIdxM) {
                for(uint resIdxN = 0; resIdxN < TN; resIdxN += 4) {

                    uint c_idx = c_interm + (threadRowWarp * TM + resIdxM) * n + threadColWarp * TN + resIdxN;
                    vec4 tmp = vec4(tensor_2[c_idx + 0], tensor_2[c_idx + 1], tensor_2[c_idx + 2], tensor_2[c_idx + 3]);
                    uint i = (wSubRowIdx * TM + resIdxM) * (WNINTER * TN) + wSubColIdx * TN + resIdxN;
                    
                    tmp.x = fma(alpha, threadResults[i + 0], beta * tmp.x);
                    tmp.y = fma(alpha, threadResults[i + 1], beta * tmp.y);
                    tmp.z = fma(alpha, threadResults[i + 2], beta * tmp.z);
                    tmp.w = fma(alpha, threadResults[i + 3], beta * tmp.w);
                    
                    tensor_2[c_idx + 0] = tmp.x;
                    tensor_2[c_idx + 1] = tmp.y;
                    tensor_2[c_idx + 2] = tmp.z;
                    tensor_2[c_idx + 3] = tmp.w;                    
                }
            }
        }
    }
   
}

#version 450
// https://www.ibiblio.org/e-notes/webgl/gpu/mul/sgemm.htm
#define WIDTH 4u                     // The vector-width (in number of floats)
#define TSM 128u                     // The tile-size in dimension M
#define TSN 128u                     // The tile-size in dimension N
#define TSK 16u                      // The tile-size in dimension K
#define WPTM 8u                      // The amount of work-per-thread in dimension M
#define WPTN 8u                      // The amount of work-per-thread in dimension N
#define LPTA ((TSK*WPTM*WPTN)/(TSN)) // The amount of loads-per-thread for A
#define LPTB ((TSK*WPTM*WPTN)/(TSM)) // The amount of loads-per-thread for B
#define RTSM 16u    // The reduced tile-size in dimension M (TSM/WPTM number of threads)
#define RTSN 16u    // The reduced tile-size in dimension N (TSN/WPTN number of threads)
#define MOD2(x,y) ((x) % (y))
#define DIV2(x,y) ((x) / (y))


layout (local_size_x = RTSM, local_size_y = RTSN, local_size_z = 1) in;
layout (std430, binding = 0) readonly buffer ssbA {
  vec4 A[];
};
layout (std430, binding = 1) readonly buffer ssbB {
  vec4 B[];
};
layout (std430, binding = 2) writeonly buffer ssbC {
  float C[];
};
uniform int param[16]; // 0:M 1:N 2:K

shared float Asub[TSK][TSM];    // Local memory to fit a tile of A and B
shared float Bsub[TSK][TSN];

void main() {
    int M = param[0];
    int N = param[1];
    int K = param[2];

    // Thread identifiers
    uint tidm = gl_LocalInvocationID.x; // Local row ID (max: TSM/WPTM == RTSM)
    uint tidn = gl_LocalInvocationID.y; // Local col ID (max: TSN/WPTN == RTSN)
    uint offsetM = TSM*gl_WorkGroupID.x; // Work-group offset
    uint offsetN = TSN*gl_WorkGroupID.y; // Work-group offset

    if (M<=offsetM) return;
    if (N<=offsetN) return;

    // Allocate register space
    float Areg;
    float Breg[WPTN];
    float acc[WPTM][WPTN];

    // Initialise the accumulation registers
    for (uint wm=0u; wm < WPTM; wm++) {
        for (uint wn=0u; wn < WPTN; wn++) {
            acc[wm][wn] = 0.0;
        }
    }
    // Loop over all tiles
    uint numTiles = K/TSK;
    uint t=0u;
    do {
        // Load one tile of A and B into local memory
        for (uint la=0u; la < LPTA/WIDTH; la++) {
            uint tid = tidn*RTSM + tidm;
            uint id = la*RTSN*RTSM + tid;
            uint row = MOD2(id,TSM/WIDTH);
            uint col = DIV2(id,TSM/WIDTH);

            // Load the values (wide vector load)
            uint tiledIndex = TSK*t + col;
            vec4 vecA = A[tiledIndex*(M/WIDTH) + offsetM/WIDTH + row];
            vec4 vecB = B[tiledIndex*(N/WIDTH) + offsetN/WIDTH + row];

            // Store the loaded vectors into local memory
            Asub[col][WIDTH*row + 0u] = vecA.x;
            Asub[col][WIDTH*row + 1u] = vecA.y;
            Asub[col][WIDTH*row + 2u] = vecA.z;
            Asub[col][WIDTH*row + 3u] = vecA.w;
            Bsub[col][WIDTH*row + 0u] = vecB.x;
            Bsub[col][WIDTH*row + 1u] = vecB.y;
            Bsub[col][WIDTH*row + 2u] = vecB.z;
            Bsub[col][WIDTH*row + 3u] = vecB.w;
        }
        // Synchronise to make sure the tile is loaded
        barrier();

        // Loop over the values of a single tile
        for (uint k=0u; k < TSK; k++) {

            // Cache the values of Bsub in registers
            for (uint wn=0u; wn < WPTN; wn++) {
                uint col = tidn + wn*RTSN;
                Breg[wn] = Bsub[k][col];
            }

            // Perform the computation
            for (uint wm=0u; wm < WPTM; wm++) {
                uint row = tidm + wm*RTSM;
                Areg = Asub[k][row];
                for (uint wn=0u; wn < WPTN; wn++) {
                    acc[wm][wn] += Areg * Breg[wn];
                }
            }
        }
        // Synchronise before loading the next tile
        barrier();

        // Next tile
        t++;
    } while (t < numTiles);

    // Store the final result in C
    for (uint wm=0u; wm < WPTM; wm++) {
        uint globalRow = offsetM + tidm + wm*RTSM;
        for (uint wn=0u; wn < WPTN; wn++) {
            uint globalCol = offsetN + tidn + wn*RTSN;
            C[globalCol*M + globalRow] = acc[wm][wn];
        }
    }
}